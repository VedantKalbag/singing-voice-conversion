{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# USE THIS NOTEBOOK AFTER TRAINING THE MODEL ON SINGING VOICE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from model_vc_og import Generator\n",
    "import torch\n",
    "import torch.nn.functional as F\n",
    "import os\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<All keys matched successfully>"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ckpt_loc = './processed_data/trained_models/checkpoint_experiment24x256x512x32_DAMP_step350000_trainloss_0.00040587130934000015.pth'\n",
    "device = 'cuda:0'\n",
    "# G = Generator(32,256,512,32).to(device)\n",
    "G = Generator(24,256,512,32)\n",
    "g_checkpoint = torch.load(ckpt_loc, map_location=device)\n",
    "G.load_state_dict(g_checkpoint['state_dict'])\n",
    "# G = torch.jit.load(ckpt_loc).to(device)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "HELPER FUNCTIONS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import pickle\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import numpy as np\n",
    "from math import ceil\n",
    "from model_vc_og import Generator\n",
    "import soundfile as sf\n",
    "import librosa\n",
    "import numpy as np\n",
    "from collections import OrderedDict\n",
    "import pyworld\n",
    "import scipy.signal\n",
    "\n",
    "def butter_highpass(cutoff, fs, order=5):\n",
    "    from scipy import signal\n",
    "    nyq = 0.5 * fs\n",
    "    normal_cutoff = cutoff / nyq\n",
    "    b, a = signal.butter(order, normal_cutoff, btype='high', analog=False)\n",
    "    return b, a\n",
    "\n",
    "def pySTFT(x, fft_length=1024, hop_length=256):\n",
    "    import numpy as np\n",
    "    from scipy.signal import get_window\n",
    "    x = np.pad(x, int(fft_length//2), mode='reflect')\n",
    "    \n",
    "    noverlap = fft_length - hop_length\n",
    "    shape = x.shape[:-1]+((x.shape[-1]-noverlap)//hop_length, fft_length)\n",
    "    strides = x.strides[:-1]+(hop_length*x.strides[-1], x.strides[-1])\n",
    "    result = np.lib.stride_tricks.as_strided(x, shape=shape,\n",
    "                                             strides=strides)\n",
    "    \n",
    "    fft_window = get_window('hann', fft_length, fftbins=True)\n",
    "    result = np.fft.rfft(fft_window * result, n=fft_length).T\n",
    "    \n",
    "    return np.abs(result)    \n",
    "    \n",
    "def get_mel_spec(x):\n",
    "    import numpy as np\n",
    "    from scipy import signal\n",
    "    from librosa.filters import mel\n",
    "    from numpy.random import RandomState\n",
    "    prng = RandomState(42)\n",
    "    mel_basis = mel(16000, 1024, fmin=90, fmax=7600, n_mels=80).T\n",
    "    min_level = np.exp(-100 / 20 * np.log(10))\n",
    "    b, a = butter_highpass(30, 16000, order=5)\n",
    "    y = signal.filtfilt(b, a, x)\n",
    "    # Ddd a little random noise for model roubstness\n",
    "    wav = y * 0.96 + (prng.rand(y.shape[0])-0.5)*1e-06\n",
    "    # Compute spect\n",
    "    D = pySTFT(wav).T\n",
    "    # Convert to mel and normalize\n",
    "    D_mel = np.dot(D, mel_basis)\n",
    "    D_db = 20 * np.log10(np.maximum(min_level, D_mel)) - 16\n",
    "    S = np.clip((D_db + 100) / 100, 0, 1)   \n",
    "    return S \n",
    "\n",
    "def get_embedding(C,x):\n",
    "    import torch\n",
    "    import numpy as np\n",
    "    len_crop = 128\n",
    "    left = np.random.randint(0, x.shape[0]-len_crop)\n",
    "    melsp = torch.from_numpy(x[np.newaxis, left:left+len_crop, : ]).cuda()\n",
    "    emb = C(melsp).detach().squeeze().cpu().numpy()\n",
    "    return emb\n",
    "\n",
    "class D_VECTOR(nn.Module):\n",
    "    \"\"\"d vector speaker embedding.\"\"\"\n",
    "    def __init__(self, num_layers=3, dim_input=40, dim_cell=256, dim_emb=64):\n",
    "        super(D_VECTOR, self).__init__()\n",
    "        self.lstm = nn.LSTM(input_size=dim_input, hidden_size=dim_cell, \n",
    "                            num_layers=num_layers, batch_first=True)  \n",
    "        self.embedding = nn.Linear(dim_cell, dim_emb)\n",
    "        \n",
    "        \n",
    "    def forward(self, x):\n",
    "        self.lstm.flatten_parameters()            \n",
    "        lstm_out, _ = self.lstm(x)\n",
    "        embeds = self.embedding(lstm_out[:,-1,:])\n",
    "        norm = embeds.norm(p=2, dim=-1, keepdim=True) \n",
    "        embeds_normalized = embeds.div(norm)\n",
    "        return embeds_normalized\n",
    "\n",
    "def pad_seq(x, base=32):\n",
    "    len_out = int(base * ceil(float(x.shape[0])/base))\n",
    "    len_pad = len_out - x.shape[0]\n",
    "    assert len_pad >= 0\n",
    "    return np.pad(x, ((0,len_pad),(0,0)), 'constant'), len_pad\n",
    "    \n",
    "def get_pyworld(wav, fs):\n",
    "    _f0, timeaxis = pyworld.dio(wav, fs)    # raw pitch extractor\n",
    "    f0 = pyworld.stonemask(wav, _f0, timeaxis, fs)  # pitch refinement\n",
    "    # Finding Spectogram\n",
    "    sp = pyworld.cheaptrick(wav, f0, timeaxis, fs)\n",
    "    # Finding aperiodicity\n",
    "    ap = pyworld.d4c(wav, f0, timeaxis, fs)\n",
    "\n",
    "    return f0, sp, ap"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "APPLYING MODEL"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def run_model(source_sp, source_emb, target_emb, ckpt_loc, device = 'cuda:0'):\n",
    "    G = Generator(32,256,512,32).to(device)\n",
    "    # G = Generator(24,256,512,24).to(device)\n",
    "    g_checkpoint = torch.load(ckpt_loc, map_location=device)\n",
    "    G.load_state_dict(g_checkpoint['state_dict'])\n",
    "    spect, len_pad = pad_seq(source_sp)\n",
    "    sp = torch.from_numpy(spect[np.newaxis, :, :]).float().to(device)\n",
    "    emb_source = torch.from_numpy(source_emb[np.newaxis, :]).float().to(device)\n",
    "    emb_target = torch.from_numpy(target_emb[np.newaxis, :]).float().to(device)\n",
    "    with torch.no_grad():\n",
    "        _, x_identic_psnt, _ = G(sp, emb_source, emb_target)\n",
    "    if len_pad == 0:\n",
    "        uttr_trg = x_identic_psnt[0].cpu().numpy()#x_identic_psnt[0, 0, :, :].cpu().numpy()\n",
    "    else:\n",
    "        uttr_trg = x_identic_psnt[0][:-len_pad].cpu().numpy()#x_identic_psnt[0, 0, :-len_pad, :].cpu().numpy()\n",
    "\n",
    "    return uttr_trg\n",
    "    # sf.write('converted_file.wav', uttr_trg, 44100)\n",
    "\n",
    "def convert_voice(source_path, target_path, ckpt_path):\n",
    "    fs = 44100\n",
    "    # LOAD AUDIO FILES\n",
    "    source, sr = librosa.load(source_path, sr=fs)\n",
    "    target, sr = librosa.load(target_path, sr=fs)\n",
    "    source = source.astype(np.float64) \n",
    "    target = target.astype(np.float64) \n",
    "\n",
    "    # EXTRACT EMBEDDINGS\n",
    "    \n",
    "    C = D_VECTOR(dim_input=80, dim_cell=768, dim_emb=256).eval().cuda()\n",
    "    c_checkpoint = torch.load('3000000-BL.ckpt')\n",
    "    new_state_dict = OrderedDict()\n",
    "    for key, val in c_checkpoint['model_b'].items():\n",
    "        new_key = key[7:]\n",
    "        new_state_dict[new_key] = val\n",
    "    C.load_state_dict(new_state_dict)\n",
    "\n",
    "    S_source = get_mel_spec(source)\n",
    "    S_source = S_source.astype(np.float32)\n",
    "\n",
    "    S_target = get_mel_spec(target)\n",
    "    S_target = S_target.astype(np.float32)\n",
    "\n",
    "    # GET EMBEDDINGS\n",
    "    source_emb = get_embedding(C, S_source)\n",
    "    target_emb = get_embedding(C,S_target)\n",
    "\n",
    "\n",
    "    # GET PYWORLD\n",
    "    f0, sp, ap = get_pyworld(source, fs=fs)\n",
    "    sp_coded = pyworld.code_spectral_envelope(sp, fs, 80)\n",
    "\n",
    "    # CONVERT VOICE\n",
    "    converted_sp_coded = run_model(sp_coded, source_emb, target_emb, ckpt_path)\n",
    "\n",
    "    converted_sp = pyworld.decode_spectral_envelope(converted_sp_coded.astype(np.double), fs, 2048)\n",
    "    # RE-SYNTHESIZE VOICE\n",
    "    y = pyworld.synthesize(f0, converted_sp, ap, fs)\n",
    "    \n",
    "    return y\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_2118922/3006723497.py:44: FutureWarning: Pass sr=16000, n_fft=1024 as keyword args. From version 0.10 passing these as positional arguments will result in an error\n",
      "  mel_basis = mel(16000, 1024, fmin=90, fmax=7600, n_mels=80).T\n"
     ]
    }
   ],
   "source": [
    "out = convert_voice('../resources/data/f1_arpeggios_straight_a.wav', '../resources/data/m8_arpeggios_straight_a.wav', './processed_data/trained_models/checkpoint_experiment32x256x512x32_DAMP_step6000_trainloss_0.0030806278809905052.pth')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import soundfile as sf\n",
    "sf.write('f1_to_m8.wav',out,44100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "sf.write('m8.wav',librosa.load('../resources/data/m8_arpeggios_straight_a.wav', sr=44100, mono=True)[0],44100)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# TESTING"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import soundfile as sf\n",
    "# sf.write('225_to_225.wav',y,22050)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# sf.write('225.wav',source,22050)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## CODED SP TESTING"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# sp_coded = pyworld.code_spectral_envelope(sp[0].cpu().numpy().astype(float), 22050, 80)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# sp[0].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# sp_coded.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# sp_decoded = pyworld.decode_spectral_envelope(sp_coded, 22050, 2048)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# sp_decoded.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_1137995/3006723497.py:44: FutureWarning: Pass sr=16000, n_fft=1024 as keyword args. From version 0.10 passing these as positional arguments will result in an error\n",
      "  mel_basis = mel(16000, 1024, fmin=90, fmax=7600, n_mels=80).T\n"
     ]
    }
   ],
   "source": [
    "# source_path='../resources/data/f1_arpeggios_straight_a.wav'\n",
    "# target_path = '../resources/data/m8_arpeggios_straight_a.wav'\n",
    "source_path = './input_data/p225/p225_001_mic1.flac'\n",
    "target_path = './input_data/p231/p231_001_mic1.flac'\n",
    "# target_path = './input_data/p225/p225_001_mic1.flac'\n",
    "source, sr = librosa.load(source_path, sr=44100)\n",
    "target, sr = librosa.load(target_path, sr=44100)\n",
    "source = source.astype(np.float64) \n",
    "target = target.astype(np.float64) \n",
    "\n",
    "# EXTRACT EMBEDDINGS\n",
    "\n",
    "C = D_VECTOR(dim_input=80, dim_cell=768, dim_emb=256).eval().cuda()\n",
    "c_checkpoint = torch.load('3000000-BL.ckpt')\n",
    "new_state_dict = OrderedDict()\n",
    "for key, val in c_checkpoint['model_b'].items():\n",
    "    new_key = key[7:]\n",
    "    new_state_dict[new_key] = val\n",
    "C.load_state_dict(new_state_dict)\n",
    "\n",
    "S_source = get_mel_spec(source)\n",
    "S_source = S_source.astype(np.float32)\n",
    "\n",
    "S_target = get_mel_spec(target)\n",
    "S_target = S_target.astype(np.float32)\n",
    "\n",
    "# GET EMBEDDINGS\n",
    "source_emb = get_embedding(C, S_source)\n",
    "target_emb = get_embedding(C,S_target)\n",
    "\n",
    "f0, sp, ap = get_pyworld(source, sr)\n",
    "coded_sp = pyworld.code_spectral_envelope(sp, sr, 80)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "spect, len_pad = pad_seq(coded_sp)\n",
    "sp = torch.from_numpy(spect[np.newaxis, :, :]).float().to(device)\n",
    "emb_source = torch.from_numpy(source_emb[np.newaxis, :]).float().to(device)\n",
    "emb_target = torch.from_numpy(target_emb[np.newaxis, :]).float().to(device)\n",
    "with torch.no_grad():\n",
    "    _, x_identic_psnt, _ = G(sp, emb_source, emb_target)\n",
    "if len_pad == 0:\n",
    "    uttr_trg = x_identic_psnt[0].cpu().numpy()#x_identic_psnt[0, 0, :, :].cpu().numpy()\n",
    "else:\n",
    "    uttr_trg = x_identic_psnt[0][:-len_pad].cpu().numpy()#x_identic_psnt[0, 0, :-len_pad, :].cpu().numpy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "sp_mod = pyworld.decode_spectral_envelope(uttr_trg.astype(np.double), sr, 1024)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "y = pyworld.synthesize(f0, sp_mod, ap, sr)\n",
    "import soundfile as sf\n",
    "sf.write('225_to_231_475kepoch.wav',y,22050)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sf.write('231.wav',target,22050)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "df = pd.read_pickle('./processed_data/pyworld/train.pkl')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "enc=[]\n",
    "for idx, row in df.iterrows():\n",
    "    enc.append(pyworld.code_spectral_envelope(row['sp'], 44100, 80))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.insert(3, 'sp_coded', enc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.to_pickle('./processed_data/pyworld/train.pkl')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "i = 34399\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "10000 < (i+1) < 100000"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "(i+1) % 10000"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "47fb57c1c385ef206c80ab929da73239cdb6d90f1675a67d5769b11ce7b14689"
  },
  "kernelspec": {
   "display_name": "Python 3.9.5 ('base')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.5"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
